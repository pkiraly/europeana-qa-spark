#!/usr/bin/env bash
#
# Run the record level measure of main features
# 
# nohup ./run-all-completeness v2018-08-completeness-ext.csv "-" --extendedFieldExtraction > v2018-08-completeness-ext.log &
#
# Input
#   The json files take place on HDFS /europeana directory
# Output
#   You should specify a file name, such as resultXX.csv
#   This will be the input of top level, and collection level aggregations


SECONDS=0
JAR_VERSION=0.6-SNAPSHOT

OUTPUT_FILE=
SKIP_FLAG=0
EXTENDED_FIELD_EXTRACTION=
VERSION=

# read the options
TEMP=`getopt -o o:sev: --long output-file:,skip-flag,extended-field-extraction,version: -n 'test.sh' -- "$@"`
eval set -- "$TEMP"

# extract options and their arguments into variables.
while true ; do
    case "$1" in
        -o|--output-file)
            case "$2" in
                "") shift 2 ;;
                *) OUTPUT_FILE=$2 ; shift 2 ;;
            esac ;;
        -s|--skip-flag) SKIP_FLAG=--skipEnrichments ; shift ;;
        -e|--extended-field-extraction) EXTENDED_FIELD_EXTRACTION=--extendedFieldExtraction ; shift ;;
        -v|--version)
            case "$2" in
                "") shift 2 ;;
                *) VERSION=$2 ; shift 2 ;;
            esac ;;
        --) shift ; break ;;
        *) echo "Internal error!" ; exit 1 ;;
    esac
done

if [[ "${OUTPUT_FILE}" == "" ]]; then
  echo "You should add an output file (such as resultXX.csv)!"
  exit 1
fi

if [ -e $OUTPUT_FILE ]; then
  echo "WARNING! ${OUTPUT_FILE} exists, rename it to prevent overwriting the file!"
  exit 1
fi

echo "Run completeness count"

source base-dirs.sh

# if hdfs dfs -test -d /result; then
#   hdfs dfs -rm -r -f -skipTrash /result
#   sleep 5
# fi

JAR=target/europeana-qa-spark-${JAR_VERSION}-jar-with-dependencies.jar

FORMAT="fullbean"

#  --inputFileName hdfs://localhost:54310/europeana/*.json \
SOURCE_DIR=file://${BASE_SOURCE_DIR}/${VERSION}/full/*.json
OUTPUT_DIR=${BASE_SOURCE_DIR}/${VERSION}/abbreviations

if [ -d ${OUTPUT_DIR} ]; then
  rm -rf ${OUTPUT_DIR}
fi

spark-submit --class de.gwdg.europeanaqa.spark.AbbreviationCheck \
  --master local[8] \
  $JAR \
  --inputFileName $SOURCE_DIR \
  --outputFileName $OUTPUT_DIR \
  --dataProvidersFile data-providers.txt \
  --datasetsFile datasets.txt \
  --format $FORMAT \
  $SKIP_FLAG $EXTENDED_FIELD_EXTRACTION

echo Retrieve $OUTPUT
# hdfs dfs -getmerge /result ${OUTPUT_FILE}
# rm .${OUTPUT_FILE}.crc
echo ${OUTPUT_DIR}/part-* | xargs cat > $OUTPUT_FILE

date +"%T"
echo "Abbreviation check is done!"

duration=$SECONDS
hours=$(($duration / (60*60)))
mins=$(($duration % (60*60) / 60))
secs=$(($duration % 60))

printf "%02d:%02d:%02d elapsed.\n" $hours $mins $secs

